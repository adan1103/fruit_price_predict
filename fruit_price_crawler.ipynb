{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:37:23.308321Z",
     "start_time": "2021-06-28T17:37:21.485667Z"
    }
   },
   "outputs": [],
   "source": [
    "# define\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from pymongo import MongoClient\n",
    "\n",
    "ss = requests.session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:15:35.997608Z",
     "start_time": "2021-06-28T17:15:35.021571Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_typhoon_alart():\n",
    "    print(\"typhoon data crawler -> start\")\n",
    "    header_typhoon = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "    \"Cookie\": \"PHPSESSID=ck51k725hgodfmfd7cbi4b0ks4; _gid=GA1.3.1183769150.1622787669; _ga=GA1.3.25450972.1620709885; _ga_K6HENP0XVS=GS1.1.1622787669.3.1.1622787695.0; TS01b0fe7f=0107dddfefcf72bbe6298d9f6067078ff9f4c14164221cb96410f497cf4481230f20f5073ca7ae71a4a5fe265de60a5c20c91db504\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\"}\n",
    "    url_typhoon = \"https://rdc28.cwb.gov.tw/TDB/public/warning_typhoon_list/get_warning_typhoon\"\n",
    "    cols_typhoon = ['颱風編號',\n",
    "         '中文名稱',\n",
    "         '英文名稱',\n",
    "         '侵臺路徑分類',\n",
    "         '海上警報開始時間',\n",
    "         '近臺強度',\n",
    "         '近臺最低氣壓(hPa)',\n",
    "         '近臺最大風速(m/s)',\n",
    "         '近臺7級風暴風半徑(km)',\n",
    "         '近臺10級風暴風半徑(km)',\n",
    "         '海上警報結束時間',       \n",
    "         '警報發布報數']\n",
    "    \n",
    "    res = ss.post(url_typhoon, headers=header_typhoon)\n",
    "    data = res.text[1:]\n",
    "    json_data = json.loads(data)\n",
    "    df = pd.json_normalize(json_data)\n",
    "#     print(df)\n",
    "\n",
    "    print(\"typhoon data crawler -> finish\")\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    typhoon = db.typhoon\n",
    "    \n",
    "    print(\"typhoon data update to mongodb -> start\")\n",
    "    for excist_id in df['id']:\n",
    "        if [x for x in typhoon.find({'id':int(excist_id)})] == []:\n",
    "            typhoon_update = df.loc[df[\"id\"]==excist_id].to_dict(orient='records')\n",
    "    #         print(typhoon_update[0])\n",
    "            updated = typhoon.insert_one(typhoon_update[0]).inserted_id\n",
    "            print(\"typhoon data update id \", updated)\n",
    "    \n",
    "    print(\"typhoon data update to mongodb -> finish\")\n",
    "    client.close() \n",
    "    \n",
    "get_typhoon_alart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:15:43.552110Z",
     "start_time": "2021-06-28T17:15:42.328544Z"
    }
   },
   "outputs": [],
   "source": [
    "def produce_year_data():\n",
    "    \n",
    "    print('produce_year_data crawler start')  \n",
    "    url = \"https://data.coa.gov.tw/Service/OpenData/DataFileService.aspx?UnitId=135\" \n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'\n",
    "            }\n",
    "\n",
    "    res = requests.get(url, headers=headers)\n",
    "    data = json.loads(res.text)\n",
    "    df = pd.json_normalize(data) # normalize => 將json檔案攤平，如有巢狀結構的話\n",
    "    print('produce_year_data crawler finish')  \n",
    "    \n",
    "    wanted_col = ['年度', '地區別', '果品類別', '收穫株數', '收穫面積_公頃', '產量_公噸']\n",
    "    df = df[wanted_col]\n",
    "    df = df.loc[~(df[\"地區別\"].isin([\"臺灣省\",\"福建省\"]))]\n",
    "\n",
    "    # 去除含有字串的資料\n",
    "    target_col = ['收穫株數', '收穫面積_公頃', '產量_公噸']\n",
    "    df.loc[(df[\"收穫面積_公頃\"].str.contains(r\"[A-Z-]\", na=False)), target_col] = 0\n",
    "    df[target_col] = df[target_col].apply(pd.to_numeric)\n",
    "\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    fruit_produce_year = db.fruit_produce_year\n",
    "\n",
    "    print(\"produce_year data update to mongodb -> start\")\n",
    "    fruit_produce_year_update = df.to_dict(orient='records')\n",
    "    updated = fruit_produce_year.insert_many(fruit_produce_year_update)\n",
    "\n",
    "    print(\"produce_year data update to mongodb -> finish\")\n",
    "    client.close() \n",
    "    return\n",
    "\n",
    "produce_year_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:15:50.803073Z",
     "start_time": "2021-06-28T17:15:50.798087Z"
    }
   },
   "outputs": [],
   "source": [
    "def title_mining(tmp_t):\n",
    "    return jieba.analyse.extract_tags(tmp_t, topK=4, withWeight=True, allowPOS=('n', 'nr', 'ns', 'nz', 'v', 'vd', 'vn'))\n",
    "\n",
    "def content_mining(tmp_c):\n",
    "    return jieba.analyse.extract_tags(tmp_c, topK=20, withWeight=True, allowPOS=('n', 'nr', 'ns', 'nz', 'v', 'vd', 'vn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:15:54.998061Z",
     "start_time": "2021-06-28T17:15:54.987085Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_jieba(function_name, ID_jieba, title_jieba, content_jieba):\n",
    "    content_list = []\n",
    "    title_list = []\n",
    "    \n",
    "    # 因為有不同文章來源，為了區別使用爬蟲網站縮寫+_id, coa=農委會, afa=農糧署\n",
    "    title_col = [function_name+'_id','key_1', 'value_1', 'key_2', 'value_2', 'key_3', 'value_3', 'key_4', 'value_4']\n",
    "    content_col = [function_name+'_id','key_1', 'value_1', 'key_2', 'value_2', 'key_3', 'value_3', 'key_4', 'value_4', 'key_5', 'value_5',\n",
    "            'key_6', 'value_6', 'key_7', 'value_7', 'key_8', 'value_8', 'key_9', 'value_9', 'key_10', 'value_10',\n",
    "            'key_11', 'value_11', 'key_12', 'value_12', 'key_13', 'value_13', 'key_14', 'value_14', 'key_15', 'value_15',\n",
    "            'key_16', 'value_16', 'key_17', 'value_17', 'key_18', 'value_18', 'key_19', 'value_19', 'key_20', 'value_20']\n",
    "\n",
    "    for i in range(len(ID_jieba)):\n",
    "        # title與content分別進入Text mining function處理\n",
    "        tmp_title = title_mining(title_jieba[i])\n",
    "        tmp_content = content_mining(content_jieba[i])\n",
    "        \n",
    "        # 清空list\n",
    "        title_keyword = []\n",
    "        content_keyword = []\n",
    "        \n",
    "        # 第一欄加入文章ID以辨識此欄位是哪篇文章\n",
    "        title_keyword.append(int(ID_jieba[i]))\n",
    "        content_keyword.append(int(ID_jieba[i]))\n",
    "        \n",
    "        # jieba分詞完是一個tuple包含分詞與詞頻的狀態，為了方便存取，將兩者拆開\n",
    "        for i in range(4):\n",
    "            # 標題\n",
    "            if i >= len(tmp_title):\n",
    "                # 若標題找到的關鍵字小於4個(topK=4)則key填入\"NA\",value填0,若這邊是空值會導致dataframe columns長度不符\n",
    "                title_keyword.append('NA')\n",
    "                title_keyword.append(0)    \n",
    "            else:\n",
    "                title_keyword.append(tmp_title[i][0]) # 分詞 str\n",
    "                title_keyword.append(tmp_title[i][1]) # 詞頻 float\n",
    "                \n",
    "        # 若內文找到的關鍵字小於20個(topK=20)則則key填入\"NA\",value填0,若這邊是空值會導致dataframe columns長度不符\n",
    "        for i in range(20):\n",
    "            # 內容\n",
    "            if i >= len(tmp_content):\n",
    "                title_keyword.append('NA')\n",
    "                title_keyword.append(0)\n",
    "            else:\n",
    "                content_keyword.append(tmp_content[i][0]) # 分詞 str\n",
    "                content_keyword.append(tmp_content[i][1]) # 詞頻 float\n",
    "        #此篇文章處理完就加入list中 -> [[文章1],[文章2]...]\n",
    "        title_list += [title_keyword]   \n",
    "        content_list += [content_keyword]\n",
    "        \n",
    "    # 建立DataFrame並return\n",
    "    df_title = pd.DataFrame(np.array(title_list), columns=title_col)\n",
    "    df_content = pd.DataFrame(np.array(content_list), columns=content_col)\n",
    "   \n",
    "    return(df_title, df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T03:59:13.425565Z",
     "start_time": "2021-06-28T03:59:13.285749Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T03:18:50.077009Z",
     "start_time": "2021-06-28T03:17:49.239621Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def afa_news(page_tmp):\n",
    "    print('afa_news crawler start')  \n",
    "    def get_info(sub_soup):\n",
    "        # 進入function後每一個try都會執行，若發生錯誤或取不到值則讓他顯示 Error方便debug\n",
    "        # 對照圖(二)單則農業新聞\n",
    "        try:               \n",
    "            title_t = sub_soup.select('div[class=\"col-sm-9\"]')[0].text\n",
    "        except:\n",
    "            title_t = 'title Error'\n",
    "        try:                \n",
    "            content_t = sub_soup.select('article[class=\"shared-content-text\"]')[0].text\n",
    "        except:\n",
    "            content_t = 'content Error'\n",
    "        try:              # text會取到\"發布日期：110-06-02\"，後面再用split切割成 [\"發布日期：\", \"110-06-02\"]，取第二個\n",
    "            post_date = sub_soup.select('div[class=\"agricultural-news-content-title row mb-lg\"]')[0].text.split('發布日期：')[1]\n",
    "        except:\n",
    "            post_date = 'post_date Error'\n",
    "\n",
    "        # return只會傳str，需要將上面取得之內容放進list內整個回傳， 否則會只回傳第一個字元     \n",
    "        tmp_list = [re.sub('[-:_、【】。；：)(「」，.&+\\n\\t\\r\\u3000]', ' ',title_t),\n",
    "                    re.sub('[-:_、【】。；：)(「」，.&+\\n\\t\\r\\u3000]', ' ', content_t),\n",
    "                    re.sub('-','/',post_date)]\n",
    "        return tmp_list[0], tmp_list[1], tmp_list[2]\n",
    "\n",
    "    url = []\n",
    "    link = []\n",
    "    out = []\n",
    "    title = []\n",
    "    content = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36',\n",
    "           }\n",
    "    org_url = 'https://www.afa.gov.tw/cht/index.php?code=list&ids=307'\n",
    "    page_tmp = 1\n",
    "    for i in range(1, page_tmp+1):\n",
    "        url.append(org_url+'&page={}'.format(i))\n",
    "\n",
    "    # 從主頁面get request並用BeautifulSoup轉換成html，用開發工具發現子頁面的連結在a標籤的'article_class'屬性中\n",
    "    # 存取該標籤的'href'，並存到link中\n",
    "    for page in range(len(url)):\n",
    "        print('page=', page+1)\n",
    "        res = ss.get(url=url[page], headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 對照圖(一)農業新聞主頁面的開發者工具\n",
    "        # 把要爬的所有子頁面的連結都先存起來\n",
    "        for i in range(0, len(soup.select('a[class=\"article_class\"]'))):\n",
    "            link.append(soup.select('a[class=\"article_class\"]')[i]['href'])\n",
    "            res_sub = ss.get(url=link[i], headers=headers)\n",
    "            soup_sub = BeautifulSoup(res_sub.text, 'html.parser')        \n",
    "    #  print('link=', link)\n",
    "\n",
    "    #將link中的article_id當成存進資料庫後的唯一識別\n",
    "    ID = list(map(lambda x: x.split('&article_id=')[1], link))    \n",
    "    # print('ID=', ID)\n",
    "\n",
    "    # 透過個連結逐一訪問子頁面\n",
    "    for j in range(len(link)):\n",
    "        # print('進入子新聞頁面', j+1)\n",
    "        res_sub = ss.get(url=link[j], headers=headers)\n",
    "        sub_soup_main = BeautifulSoup(res_sub.text, 'html.parser')\n",
    "\n",
    "        # 將BeautifulSoup處理過的html代入函式處理，主要程式流程看起來比較乾淨\n",
    "        # html帶入get_info執行完會回傳3個str，分別是標題、內容、發布日期，因此我們要用3個變數暫存再放進list中\n",
    "        title_tmp, content_tmp, tmp_date = get_info(sub_soup_main)\n",
    "        # 將多個空白改成一個空白 \n",
    "        title.append((' '.join(title_tmp.split())))\n",
    "        content.append((' '.join(content_tmp.split())))\n",
    "        out.append([int(ID[j]), tmp_date, (' '.join(title_tmp.split())), (' '.join(content_tmp.split())), link[j]])\n",
    "    print('afa_news crawler finish')  \n",
    "\n",
    "    print('afa_news Text mining start')\n",
    "    df_title, df_content = news_jieba(\"afa\", ID, title, content)\n",
    "    print('afa_news Text mining finish')\n",
    "\n",
    "    # 建立連線並定義collections名稱\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    afa_news_title_jieba = db.afa_news_title_jieba\n",
    "    afa_news_content_jieba = db.afa_news_content_jieba\n",
    "    afa_news = db.afa_news\n",
    "\n",
    "    # 定義數字型態的columns並將該欄位所有rows轉換成數字型態\n",
    "    news_col = ['afa_id', 'date', 'title', 'content', 'link']\n",
    "    title_cols = ['afa_id','value_1', 'value_2', 'value_3', 'value_4']\n",
    "    content_cols = ['afa_id', 'value_1','value_2', 'value_3','value_4', 'value_5',\n",
    "                   'value_6', 'value_7',  'value_8',  'value_9',  'value_10',\n",
    "                  'value_11', 'value_12', 'value_13', 'value_14', 'value_15',\n",
    "                  'value_16', 'value_17', 'value_18', 'value_19', 'value_20']\n",
    "    df_afa_news = pd.DataFrame(np.array(out), columns=news_col)\n",
    "\n",
    "    df_title[title_cols] = df_title[title_cols].apply(pd.to_numeric)\n",
    "    df_content[content_cols] = df_content[content_cols].apply(pd.to_numeric)\n",
    "    df_afa_news['afa_id'] = df_afa_news['afa_id'].apply(pd.to_numeric)\n",
    "\n",
    "    \n",
    "    print(\"afa_news update to mongodb -> start\")\n",
    "    # 判斷id是否存在於mongodb中，若無則寫進資料庫\n",
    "    for excist_id in df_afa_news['afa_id']:\n",
    "        if [x for x in afa_news.find({\"afa_id\":int(excist_id)})] == []:\n",
    "            afa_news_update = df_afa_news.loc[df_afa_news[\"afa_id\"]==excist_id].to_dict(orient='records')\n",
    "            updated = afa_news.insert_one(afa_news_update[0]).inserted_id\n",
    "            print(\"afa_news update id \", updated)\n",
    "\n",
    "    for excist_id in df_title['afa_id']:\n",
    "        if [x for x in afa_news_title_jieba.find({\"afa_id\":int(excist_id)})] == []:\n",
    "            afa_news_title_jieba_update = df_title.loc[df_title[\"afa_id\"]==excist_id].to_dict(orient='records')\n",
    "            # print(afa_news_title_jieba_update[0])\n",
    "            updated = afa_news_title_jieba.insert_one(afa_news_title_jieba_update[0]).inserted_id\n",
    "            print(\"afa_news title jieba update id \", updated)\n",
    "\n",
    "\n",
    "    for excist_id in df_content['afa_id']:\n",
    "        if [x for x in afa_news_content_jieba.find({\"afa_id\":int(excist_id)})] == []:\n",
    "            afa_news_content_jieba_update = df_content.loc[df_content[\"afa_id\"]==excist_id].to_dict(orient='records')\n",
    "            updated = afa_news_content_jieba.insert_one(afa_news_content_jieba_update[0]).inserted_id\n",
    "            print(\"afa_news content jieba update id \", updated)\n",
    "\n",
    "    print(\"afa_news update to mongodb -> finish\")\n",
    "    client.close()\n",
    "\n",
    "    return\n",
    "\n",
    "afa_news(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T11:15:45.593704Z",
     "start_time": "2021-06-28T11:13:39.842953Z"
    }
   },
   "outputs": [],
   "source": [
    "def coa_news(start_year_tmp, start_month_tmp, end_year_tmp, end_month_tmp):\n",
    "    print('coa_news crawler start') \n",
    "    form_data = {\n",
    "    'keyword': '',\n",
    "    'division_lv1': '*',\n",
    "    'year': start_year_tmp,\n",
    "    'month': start_month_tmp,\n",
    "    'end_year': end_year_tmp,\n",
    "    'end_month': end_month_tmp,\n",
    "    'search_Submit': '查詢',\n",
    "    'is_search': 'y'\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36',\n",
    "    }\n",
    "    url = 'https://www.coa.gov.tw/theme_list.php?theme=news&sub_theme=agri'\n",
    "    ss = requests.session()\n",
    "    res = ss.post(url=url, headers=headers, data=form_data)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    out = []\n",
    "    ID = []\n",
    "    date = []\n",
    "    title = []\n",
    "    author = []\n",
    "    link = []\n",
    "    content = []\n",
    "\n",
    "    # 取發布日期\n",
    "    for b in range(1, len(soup.select('td[align=\"center\"]')), 3):\n",
    "        newsDate = soup.select('td[align=\"center\"]')[b]\n",
    "        date.append(newsDate.text)\n",
    "\n",
    "    # 取發布機關\n",
    "    for c in range(2, len(soup.select('td[align=\"center\"]')), 3):\n",
    "        newsAuthor = soup.select('td[align=\"center\"]')[c]\n",
    "        author.append(newsAuthor.text)\n",
    "\n",
    "    # 取新聞標題、網址\n",
    "    for i in range(0, len(soup.select('a[class=\"main-c9-index\"]'))):\n",
    "        newsTitle = soup.select('a[class=\"main-c9-index\"]')[i]['title']\n",
    "        newsLink = 'https://www.coa.gov.tw/' + soup.select('a[class=\"main-c9-index\"]')[i]['href']\n",
    "        title.append(re.sub('[-:_【】)(「」&+\\n\\t\\r\\u3000\\xa0]', ' ', newsTitle))\n",
    "        link.append(newsLink)\n",
    "\n",
    "    # 取新聞內容、文號\n",
    "    for j in range(len(link)):\n",
    "        page_res = ss.get(url=link[j], headers=headers)\n",
    "        page_soap = BeautifulSoup(page_res.text, 'html.parser')\n",
    "\n",
    "        # 災情報告文號為\"HOT\"，改取連結後面的id\n",
    "        ID_tmp = page_soap.select('td[class=\"word-2\"]')[0].text.split(\"：\")[1]\n",
    "        if ID_tmp.isdigit():\n",
    "            ID.append(ID_tmp)\n",
    "        else:\n",
    "            ID.append(link[j].split(\"&id=\")[1])\n",
    "\n",
    "        for w in page_soap.select('div[class=\"word\"]'):\n",
    "            content.append(re.sub('[-:_【】)(「」&+\\n\\t\\r\\u3000\\xa0]', ' ', w.text))\n",
    "\n",
    "    for k in range(len(ID)):\n",
    "        out.append([ID[k], date[k], author[k], title[k], content[k], link[k]])\n",
    "    print('out=', out)\n",
    "    print('coa_news crawler finish')  \n",
    "\n",
    "    # 定義數字型態的columns並將該欄位所有rows轉換成數字型態\n",
    "    news_col = ['coa_id', 'date', 'author', 'title', 'content', 'link']\n",
    "    title_cols = ['coa_id','value_1', 'value_2', 'value_3', 'value_4']\n",
    "    content_cols = ['coa_id', 'value_1','value_2', 'value_3','value_4', 'value_5',\n",
    "                   'value_6', 'value_7',  'value_8',  'value_9',  'value_10',\n",
    "                  'value_11', 'value_12', 'value_13', 'value_14', 'value_15',\n",
    "                  'value_16', 'value_17', 'value_18', 'value_19', 'value_20']\n",
    "\n",
    "    df_coa_news = pd.DataFrame(np.array(out), columns=news_col)\n",
    "    df_coa_news['coa_id'] = df_coa_news['coa_id'].apply(pd.to_numeric)\n",
    "\n",
    "    # text mining\n",
    "    print('coa_news Text mining start')\n",
    "    df_title, df_content = news_jieba(\"coa\", ID, title, content)\n",
    "    print('coa_news Text mining finish')\n",
    "\n",
    "    # 建立連線並定義collections名稱\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    coa_news_title_jieba = db.coa_news_title_jieba\n",
    "    coa_news_content_jieba = db.coa_news_content_jieba\n",
    "    coa_news = db.coa_news\n",
    "\n",
    "\n",
    "    df_title[title_cols] = df_title[title_cols].apply(pd.to_numeric)\n",
    "    df_content[content_cols] = df_content[content_cols].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "    # 判斷id是否存在於mongodb中，若無則insert\n",
    "    print(\"coa_news update to mongodb -> start\")\n",
    "\n",
    "    for excist_id in df_coa_news['coa_id']:\n",
    "        if [x for x in coa_news.find({\"coa_id\":int(excist_id)})] == []:\n",
    "            coa_news_update = df_coa_news.loc[df_coa_news[\"coa_id\"]==excist_id].to_dict(orient='records')\n",
    "            updated = coa_news.insert_one(coa_news_update[0]).inserted_id\n",
    "            print(\"coa_news update id \", updated)\n",
    "\n",
    "    for excist_id in df_title['coa_id']:\n",
    "        if [x for x in coa_news_title_jieba.find({\"coa_id\":int(excist_id)})] == []:\n",
    "            coa_news_title_jieba_update = df_title.loc[df_title[\"coa_id\"]==excist_id].to_dict(orient='records')\n",
    "    #         print(afa_news_title_jieba_update[0])\n",
    "            updated = coa_news_title_jieba.insert_one(coa_news_title_jieba_update[0]).inserted_id\n",
    "            print(\"coa_news title jieba update id \", updated)\n",
    "\n",
    "\n",
    "    for excist_id in df_content['coa_id']:\n",
    "        if [x for x in coa_news_content_jieba.find({\"coa_id\":int(excist_id)})] == []:\n",
    "            coa_news_content_jieba_update = df_content.loc[df_content[\"coa_id\"]==excist_id].to_dict(orient='records')\n",
    "            updated = coa_news_content_jieba.insert_one(coa_news_content_jieba_update[0]).inserted_id\n",
    "            print(\"coa_news content jieba update id \", updated)\n",
    "\n",
    "    print(\"coa_news update to mongodb -> finish\")\n",
    "    client.close()\n",
    "\n",
    "    return\n",
    "\n",
    "coa_news(110, 4, 110, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T11:06:52.711469Z",
     "start_time": "2021-06-28T11:06:52.701496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marketing_price_soup(soup):\n",
    "    cnt = 0\n",
    "    table = soup.select(\"table[border='1']\")\n",
    "    table_content = list(filter(None,table[0].text.split('\\n')))\n",
    "    Header = table_content[0:10]\n",
    "    Header[7] = '價格跟前一交易日比較%'\n",
    "    Header[9] = '交易量跟前一交易日比較%'\n",
    "    print('marketing_price crawler -> finish') \n",
    "    \n",
    "    #清洗資料\n",
    "    data = table_content[18:]\n",
    "    output=[]\n",
    "    for s_data in range(0, len(data), 10):\n",
    "        output.append(data[s_data:s_data+10])    \n",
    "    df3 = pd.DataFrame(output,columns=Header)\n",
    "    # 按日期排序，reset_index後會多出index欄位，後面讀回的原始資料沒有index因此一併去除\n",
    "    df3 = df3.sort_values([\"日期\"], ascending=True).reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "    # 將交易量中的\",\"去除並轉成數字\n",
    "    df3[\"交易量(公斤)\"] = df3[\"交易量(公斤)\"].apply(lambda x: int(\" \".join(re.sub(\",\", \"\", x).split())))\n",
    "    \n",
    "    # dataFrame部分欄位轉成數字型態\n",
    "    col = [\"上價\", \"中價\", \"下價\", \"平均價(元/公斤)\", \"交易量(公斤)\"]\n",
    "    df3[col] = df3[col].apply(pd.to_numeric)\n",
    "    \n",
    "    print(\"marketing_price update to mongodb -> start\")\n",
    "    # 寫進mongodb\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    marketing_price_data = db.marketing_price_data\n",
    "    \n",
    "    #用日期與市場欄位判斷資料是否已存在，若無則寫入資料庫\n",
    "    for excist_time, excist_marketing in zip(df3['日期'], df3['市場']):\n",
    "#         print(excist_time,excist_marketing)\n",
    "        if [x for x in marketing_price_data.find({\"日期\":excist_time, \"市場\":excist_marketing})] == []:\n",
    "            marketing_price_data_update = df3.loc[(df3[\"日期\"]==excist_time) & (df3[\"市場\"]==excist_marketing)].to_dict(orient='records')\n",
    "            updated = marketing_price_data.insert_one(marketing_price_data_update[0]).inserted_id\n",
    "            print(\"marketing_price_data update id \", updated)\n",
    "    print(\"marketing_price update to mongodb -> finish\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marketing_price(fruit, start_date):\n",
    "    \n",
    "    print('marketing_price crawler -> start') \n",
    "    fruit_select = {\"香蕉\":\"A1\",\"鳳梨\":\"B2\"}\n",
    "    url = \"https://amis.afa.gov.tw/fruit/FruitProdDayTransInfo.aspx\"\n",
    "\n",
    "    driver = Chrome(\"./chromedriver\")\n",
    "    driver.get(url)\n",
    "\n",
    "    #### 1. 選取範圍 => 期間\n",
    "    driver.find_element_by_xpath(\"//*[@id='ctl00_contentPlaceHolder_ucDateScope_rblDateScope_1']\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    #### 2. 選取日期\n",
    "    # 執行js語法來解除只能read的input格子\n",
    "    driver.execute_script(\"$('input[id=ctl00_contentPlaceHolder_txtSTransDate]').removeAttr('readonly')\")\n",
    "\n",
    "    # 清空既有input並放入keys\n",
    "    driver.find_element_by_id('ctl00_contentPlaceHolder_txtSTransDate').clear() \n",
    "    driver.find_element_by_id('ctl00_contentPlaceHolder_txtSTransDate').send_keys(start_date)\n",
    "    # time.sleep(2)\n",
    "\n",
    "    #### 3. 選取市場(目前code僅能選全部市場)\n",
    "    driver.find_element_by_xpath(\"//*[@id='ctl00_contentPlaceHolder_txtMarket']\").click() \n",
    "    #time.sleep(3)\n",
    "\n",
    "    # 點選後轉移到跳出視窗，選取全部市場\n",
    "    iframe = driver.find_elements_by_tag_name(\"iframe\")[0]\n",
    "    driver.switch_to.frame(iframe)\n",
    "    radio_target = driver.find_element_by_xpath(\"//*[@id='radlMarketRange_0']\")\n",
    "    radio_target.click()\n",
    "\n",
    "    #### 4. 選取水果種類 \n",
    "    driver.find_element_by_xpath(\"//*[@id='ctl00_contentPlaceHolder_txtProduct']\").click()\n",
    "    #time.sleep(3)\n",
    "\n",
    "    # 點選後轉移到跳出視窗\n",
    "    iframe = driver.find_elements_by_tag_name(\"iframe\")[0]\n",
    "    driver.switch_to.frame(iframe)\n",
    "\n",
    "    # 抓取下拉選單元件，直接以值來選擇\n",
    "    select = Select(driver.find_element_by_name('lstProduct'))\n",
    "    select.select_by_value(fruit_select[fruit])\n",
    "\n",
    "    # 選取完成後，關閉視窗\n",
    "    driver.find_element_by_xpath(\"//*[@id='btnConfirm']\").click()\n",
    "\n",
    "    #### 5. 點選查詢button\n",
    "    driver.find_element_by_xpath(\"//*[@id='ctl00_contentPlaceHolder_btnQuery']\").click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "#     html = driver.execute_script(\"return document.getElementsByTagName('html')[0].outerHTML\")\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "    marketing_price_soup(soup)\n",
    "    \n",
    "marketing_price(\"香蕉\", '110/05/01')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理中\n",
    "\n",
    "def wether(StartYear):\n",
    "    \"\"\"\n",
    "    resource_path = r'./WeatherDatas'\n",
    "    if not os.path.exists(resource_path):\n",
    "        os.mkdir(resource_path)\n",
    "\n",
    "    today = datetime.datetime.now()\n",
    "    NowYear = today.year\n",
    "    NowMonth = today.month\n",
    "    NowDay = today.day\n",
    "\n",
    "    Error = []\n",
    "    with open('./現存測站.csv',encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        column = [row for row in reader]\n",
    "        print('column=', column)\n",
    "        for i in range(0,len(column)):\n",
    "            StationNumber = column[i]['站號']\n",
    "            StationName = column[i]['站名']\n",
    "            StationLocation = column[i]['城市']\n",
    "            # StartYear = column[i]['資料起始日期'].split('/')[0]\n",
    "            # StartMonth = column[i]['資料起始日期'].split('/')[1]\n",
    "            \n",
    "            Yearlist = []\n",
    "            Monthlist = []\n",
    "\n",
    "            # 取得每個測站起始時間\n",
    "            for year in range(StartYear, NowYear + 1):\n",
    "                if year == NowYear:\n",
    "                    for j in range(1, NowMonth + 1):\n",
    "                        if len(str(j)) == 1:\n",
    "                            month = \"0\" + str(j)\n",
    "                        else:\n",
    "                            month = j\n",
    "                        Yearlist.append(year)\n",
    "                        Monthlist.append(month)\n",
    "                else:\n",
    "                    for j in range(1, 13):\n",
    "                        if len(str(j)) == 1:\n",
    "                            month = \"0\" + str(j)\n",
    "                        else:\n",
    "                            month = j\n",
    "                        Yearlist.append(year)\n",
    "                        Monthlist.append(month)\n",
    "\n",
    "            #按各年、月份分別爬蟲\n",
    "            print('======{} is START!======'.format(StationName))\n",
    "            for k in range(0, len(Yearlist)):\n",
    "                print(\"==========================\")\n",
    "                print(Yearlist[k], Monthlist[k])\n",
    "                url = \"http://e-service.cwb.gov.tw/HistoryDataQuery/MonthDataController.do?command=viewMain&station={}&stname=%25E9%259E%258D%25E9%2583%25A8&datepicker={}-{}\".format(StationNumber, Yearlist[k], Monthlist[k])\n",
    "                r = requests.get(url)\n",
    "                r.encoding = \"utf-8\"\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                tag_table = soup.find(id=\"MyTable\")\n",
    "                rows = tag_table.findAll(\"tr\")\n",
    "                rowList = [[cell.get_text().replace(\"\\n\", \"\").replace(\"\\r\", \"\") for cell in row.findAll([\"td\"])] for row in rows]\n",
    "\n",
    "                resource_path2 = r'{}/{}'.format(resource_path, StationLocation)\n",
    "                if not os.path.exists(resource_path2):\n",
    "                    os.mkdir(resource_path2)\n",
    "                resource_path3 = r'{}/{}'.format(resource_path2, StationName)\n",
    "                if not os.path.exists(resource_path3):\n",
    "                    os.mkdir(resource_path3)\n",
    "\n",
    "                try:\n",
    "                    df = pd.DataFrame(rowList,\n",
    "                                      columns=['ObsTime', 'StnPres', 'SeaPres', 'StnPresMax', 'StnPresMaxTime', 'StnPresMin',\n",
    "                                               'StnPresMinTime', 'Temperature', 'T Max', 'T Max Time', 'T Min', 'T Min Time',\n",
    "                                               'Td dew point', 'RH', 'RHMin', 'RHMinTime', 'WS', 'WD', 'WSGust', 'WDGust',\n",
    "                                               'WGustTime', 'Precp', 'PrecpHour', 'PrecpMax10', 'PrecpMax10Time', 'PrecpMax60',\n",
    "                                               'PrecpMax60Time', 'SunShine', 'SunShineRate', 'GloblRad', 'VisbMean', 'EvapA',\n",
    "                                               'UVI Max', 'UVI Max Time', 'Cloud Amount'])\n",
    "\n",
    "\n",
    "                    df2 = df.copy()\n",
    "                    df3 = df2.drop(df2.index[[0, 1, 2]])\n",
    "                    df3.to_csv(r'%s/%s.csv' % (resource_path3, StationName + \"{}{}\".format(Yearlist[k], Monthlist[k])),index=False)\n",
    "                    print('OK: {}/{}'.format(Yearlist[k], Monthlist[k]))\n",
    "                    time.sleep(2)\n",
    "                except ValueError as e:\n",
    "                    print(\"ValueError:\", url)\n",
    "                    Error.append(url)\n",
    "                except IndexError as e:\n",
    "                    print(\"IndexError:\", url)\n",
    "                    Error.append(url)\n",
    "\n",
    "                print(\"==========================\")\n",
    "            print('======{} is over======'.format(StationName))\n",
    "            time.sleep(2)\n",
    "    return\n",
    "with open('{}/output.txt'.format(resource_path), \"w\") as f:\n",
    "    for error in Error:\n",
    "        f.write('{}\\n'.format(error))\n",
    "\"\"\"    \n",
    "wether(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:30:59.370421Z",
     "start_time": "2021-06-28T15:30:37.184779Z"
    }
   },
   "outputs": [],
   "source": [
    "def origin_price(StartYear_tmp, StartMonth_tmp, EndYear_tmp, EndMonth_tmp):\n",
    "    \n",
    "    print('origin_price crawler -> start') \n",
    "    url = \"https://apis.afa.gov.tw/pagepub/AppContentPage.aspx?itemNo=PRI075\"\n",
    "    driver = Chrome('./chromedriver')\n",
    "    driver.get(url)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"//*[@id='PRI105']\").click()\n",
    "    time.sleep(5)  \n",
    "    ###選旬平均\n",
    "    driver.find_element_by_xpath(\"//*[@id='WR1_2_Q_AvgPriceType_C1_2']\").click()\n",
    "    time.sleep(3)\n",
    "    ###開始年\n",
    "    select = Select(driver.find_element_by_name('WR1_2$Q_PRSR_Year1$C1'))\n",
    "    select.select_by_visible_text(u\"{}\".format(StartYear_tmp))\n",
    "    ###開始月\n",
    "    time.sleep(1)\n",
    "    select = Select(driver.find_element_by_name('WR1_2$Q_PRSR_Month1$C1'))\n",
    "    select.select_by_visible_text(u\"{}\".format(StartMonth_tmp))\n",
    "    ###結束年\n",
    "    time.sleep(1)\n",
    "    select = Select(driver.find_element_by_name('WR1_2$Q_PRSR_Year2$C1'))\n",
    "    select.select_by_visible_text(u\"{}\".format(EndYear_tmp))\n",
    "    ###結束月\n",
    "    time.sleep(1)\n",
    "    select = Select(driver.find_element_by_name('WR1_2$Q_PRSR_Month2$C1'))\n",
    "    select.select_by_visible_text(u\"{}\".format(EndMonth_tmp))\n",
    "    ###選種類\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(\"//input[@id='WR1_2_PRMG_02_23']\").click()\n",
    "    ###查詢\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(\"//*[@class='CSS_ABS_NormalLink']\").click()\n",
    "\n",
    "    time.sleep(2)\n",
    "    window_1 = driver.current_window_handle\n",
    "    windows = driver.window_handles\n",
    "    for current_window in windows:\n",
    "        if current_window != window_1:\n",
    "            driver.switch_to.window(current_window)\n",
    "\n",
    "    time.sleep(2)      \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        table = soup.select(\"table[border='1']\")\n",
    "    except:\n",
    "        print('production price Error')\n",
    "\n",
    "    rows = table[0].findAll(\"tr\")\n",
    "    rowList = [[cell.get_text().replace(\"\\n\", \"\") for cell in row.findAll([\"td\"])] for row in rows]\n",
    "    Header = rowList[0]\n",
    "    Content = rowList[1:len(rowList)]\n",
    "    df = pd.DataFrame(Content, columns=Header)\n",
    "    print('origin_price crawler -> finish')\n",
    "    \n",
    "    # 初步資料清潔\n",
    "    df_transposed = df.set_index('地點').T\n",
    "    df_transposed = df_transposed.reset_index()\n",
    "    df_transposed = df_transposed.rename(columns={\"index\": \"時間\"})\n",
    "    \n",
    "    col = []\n",
    "    for i in range(1, len(df_transposed.columns)):\n",
    "        col.append(df_transposed.columns[i])\n",
    "    \n",
    "    df_transposed[df_transposed[col] == '-'] = 0\n",
    "    df_transposed[col] = df_transposed[col].apply(pd.to_numeric)\n",
    "    # 清掉->(元/公斤)\n",
    "    df_transposed[\"時間\"] = df_transposed[\"時間\"].apply(lambda x: x.split(\"(\")[0])\n",
    "    print(df_transposed.head())\n",
    "\n",
    "    print(\"origin_price update to mongodb -> start\")\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    origin_price_data = db.origin_price_data\n",
    "\n",
    "    for excist_time in df_transposed['時間']:\n",
    "        if [x for x in origin_price_data.find({\"時間\":excist_time})] == []:\n",
    "            origin_price_data_update = df_transposed.loc[df_transposed[\"時間\"]==excist_time].to_dict(orient='records')\n",
    "            updated = origin_price_data.insert_one(origin_price_data_update[0]).inserted_id\n",
    "            print(\"origin_price_data update id \", updated)\n",
    "    client.close()       \n",
    "    print(\"origin_price update to mongodb -> finish\")\n",
    "    \n",
    "    \n",
    "origin_price(2020, 1, 2020, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:09:17.655106Z",
     "start_time": "2021-06-28T15:09:17.632137Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:58:14.778653Z",
     "start_time": "2021-06-28T17:58:11.329694Z"
    }
   },
   "outputs": [],
   "source": [
    "# code是正常的，但不知道為啥有一次執行jupyter直接掛掉@@\n",
    "def agriculture_survey():\n",
    "    print(\"agriculture_survey crawler start\")\n",
    "    url = \"https://data.coa.gov.tw/Service/OpenData/FromM/TownCropData.aspx\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'\n",
    "            }\n",
    "\n",
    "    res = requests.get(url, headers=headers)\n",
    "    data = json.loads(res.text)\n",
    "    df = pd.json_normalize(data)\n",
    "    print(\"agriculture_survey crawler finish\")\n",
    "\n",
    "    print(\"agriculture_survey update to mongodb -> start\")\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    agriculture_survey_data = db.agriculture_survey_data\n",
    "\n",
    "    agriculture_survey_update = df.to_dict(orient='records')\n",
    "    agriculture_survey_data.insert_many(agriculture_survey_update)\n",
    "    print(\"agriculture_survey update to mongodb -> finish\")\n",
    "    \n",
    "agriculture_survey()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:43:10.185009Z",
     "start_time": "2021-06-28T17:43:09.681451Z"
    }
   },
   "outputs": [],
   "source": [
    "def Fruit_season():\n",
    "    print(\"Fruit_season crawler start\")\n",
    "    url = \"https://data.coa.gov.tw/Service/OpenData/DataFileService.aspx?UnitId=061&$top=6000&$skip=0\" # \"https://data.coa.gov.tw/Service/OpenData/DataFileService.aspx?UnitId=113\" #\"https://data.coa.gov.tw/Service/OpenData/FromM/TownCropData.aspx\"\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'\n",
    "                }\n",
    "\n",
    "    res = requests.get(url, headers=headers)\n",
    "    data = json.loads(res.text)\n",
    "    print(\"Fruit_season crawler finish\")\n",
    "    \n",
    "    df = pd.json_normalize(data)\n",
    "    df_fruit = df.loc[df[\"type\"] == '水果']\n",
    "    df_fruit[\"month\"] = df_fruit[\"month\"].astype(\"int\")\n",
    "    \n",
    "    print(\"Fruit_season update to mongodb -> start\")\n",
    "    client = MongoClient()\n",
    "    db = client.test\n",
    "    Fruit_season_data = db.Fruit_season_data\n",
    "\n",
    "    df_fruit_data_update = df_fruit.to_dict(orient='records')\n",
    "    Fruit_season_data.insert_many(df_fruit_data_update)\n",
    "    print(\"Fruit_season update to mongodb -> finish\")\n",
    "    \n",
    "Fruit_season()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "#select crawler\n",
    "\n",
    "# 水果產季\n",
    "Fruit_season()\n",
    "\n",
    "# 農情調查\n",
    "agriculture_survey()\n",
    "\n",
    "# 颱風警報\n",
    "get_typhoon_alart()\n",
    "\n",
    "# 年度生產愾況\n",
    "produce_year() \n",
    "\n",
    "# 市場價格 (種類, 開始日期(民國年/月/日))\n",
    "marketing_price(\"香蕉\", '110/05/01')\n",
    "\n",
    "# 新聞 (開始年, 開始月, 結束年, 結束月)\n",
    "coa_news(110, 1, 110, 6)\n",
    "\n",
    "# 新聞 (總爬取頁數)\n",
    "afa_news(2) \n",
    "\n",
    "# 氣象資料(開始年分)\n",
    "wether(2021)\n",
    "\n",
    "# 產地價格\n",
    "origin_price(2020, 1, 2020, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T17:39:20.276040Z",
     "start_time": "2021-06-28T17:39:20.172900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://<username>:<password>@twfruit.i2omj.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
    "db = client.TWFruits\n",
    "test = db.test\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
